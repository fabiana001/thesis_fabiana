
The World Wide Web, in its current form, is modern legacy system where data cannot be automatically accessed and manipulated. Goal of Web Mining is therefore to discover and to extract valuable information in the way to improve and automate the information gathering process. 

In this thesis I face the problem of mining structured data encoded in web pages or hidden in the topological structure of a website. 
 This is due analyzing the most important representations of a web page (i.e. textual, visual, structural representation) and combining the extracted intra-page information with inter-page information (i.e. hyperlink structure).  
In particular this thesis focus on two main open challenges, that is combining structured data splitted on multiple web pages and organizing web pages semantically similar.

In the first case,  similar to databases, where a view can represent a subset of the data contained in a table, websites split a logical list in multiple views in order to avoid information overload and to facilitate users' navigation. However, the data stored in such \emph{logical list} need to be automatically extracted to enable building services for market intelligence, synonyms discovery, question answering and data mashup. The approach presented in this thesis solves this open issue.
Experimental results show that the algorithm is extremely accurate and it is able to extract \textit{logical lists} in a wide range of domains and websites with high precision and recall.
Part of this future work will involve tasks such as indexing the Web based on lists and tables, answering queries from lists, and entity discovery and disambiguation using lists.

In the second case, web pages can be grouped based on different types of features. To solve this open issue, I present two unsupervised and domain-independent approaches. In particular, the first method combines information about content, web page structure and hyperlink structure in a single vector space representation which can be used by any traditional and best-performing clustering algorithms. To take into account the hyperlink structure I exploit recent advances in natural language processing by adapting the skip-gram model. %In the evaluation I have analyzed two research questions: 1) Which is the real contribution of  combining  content  and  hyperlink  structure  in  a  single vector space representation with respect to using only either textual content or hyperlink structure? 2) Which is the real contribution  of  exploiting  web  pages  structure  (i.e.  HTML formatting)  and,  specifically,  the  role  of  using  web  lists  to reduce noise and improve clustering results? 
Experiments results show that content and hyperlink structure of web pages provide  different  and  complementary  information which can improve the efficacy of clustering algorithms. Moreover, experiments do not show statistical differences between results which use web lists and results obtained ignoring web page structure. 
The second approach proposed in this thesis is related with the automatic sitemap generation. A sitemap represents an explicit specification of the design concept and knowledge organization of a website. Intuitively, sibling Web pages this hierarchy should be of the same type (e.g. professors web pages, courses web pages, etc.), and parents and children pages should be of more general and more specific types respectively. For this purpose I propose a new method which automatically discovers the hierarchical organization of a website by using the content and the hyperlinks structure of web pages. Experimental results prove that the proposed approach outperforms HDTM, a state-of-art algorithm which extract the hierarchical structure of a website through the analysis of contents and hyperlinks 
