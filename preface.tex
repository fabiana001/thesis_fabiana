% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ./thesis.tex
% !TEX spellcheck = en
%************************************************

%This thesis is the final outcome of my Ph.D. study at Faculty of Computer Science, University of Bari. It serves as documentation of my research work, which has been done between January 2013 and  September 2016.  The thesis consists of five chapters.

%In the first chapter I analyze the problem of automated information gathering on the Web, using structured data encoded or hidden in web pages or in the websites hyperlink structure. Therefore, for this purpose, the main features of the Web are described and the 

The World Wide Web is the largest and most widely known repository of hypertext. Hypertextual documents (also called web documents or web pages) contain information about every topic and every real-world entity, authored and edited by millions of people and written in hundreds of languages. However, this repository can be considered as a modern legacy system because such stored data cannot be easily accessed and manipulated.

Web mining, that is  the process of information discovery from sources across the World Wide Web, is one of the hottest topic into the Computer Science community.  Although Web Mining borrows heavily from traditional fields such as Information Retrieval, Data Mining, Statistics, etc., the characteristics of the Web (e.g. heterogeneity, noise, dynamicity, dimension, etc. ) make techniques and approaches used in these fields non directly applicable on data such as web pages or websites. Moreover, extracted information, in form of structured data, are very valuable both for improving the performances of existing applications (e.g. improving search engine results) and for generating new applications (e.g. automatic sitemaps generation, breadcrumb mining, etc.)

In my thesis, I investigate the principles and methodologies for extracting structured data from websites, merging structured data spanned on multiple web pages and organizing web pages based on structured data. This is achieved through the development of models and algorithms  which exploit multiple web page representations (e.g. textual, visual, structural, etc.) and combine these information among them.
