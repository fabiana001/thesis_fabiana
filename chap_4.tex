% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ./thesis.tex
% !TEX spellcheck = en

%************************************************

The process of automatically organizing web pages and websites has always attracted extensive attention in several research areas because of its significant theoretical challenges and because of its great application and commercial potentials.
Clustering represents one of the most investigated techniques used to evaluate the interaction among web pages and organize them into groups such that web pages within the same group are more similar to each other than those belonging to different ones. As a consequence, clustering algorithms can be profitably used to organize the 
content of websites and to understand their hidden structure or, in Information Retrieval, to provide an effective representation of search engine results \cite{Zamir:1998}. 

Since a web page is characterized by several representations (i.e. textual representation,  structural representation based on HTML tags and visual features and structural representation based on hyperlinks) the existing clustering algorithms differ in their ability of using these representations.  

%Existing clustering algorithms used to group web pages typically organize them using either their content or their structure. To achieve this goal, clustering algorithms are able to exploit the content and/or the structural similarity between pages. 

In particular, clustering based on textual representation typically group web pages using words distribution~\cite{Chehreghani:2008, Haveliwala:2002, Zamir:1998}. 
%Solutions based on this approach manage web pages as plain text corpora, even if it is known that web pages contain richer and, sometimes, implicit information associated with them (e.g. their structure). 
Solutions based on this approach manage web pages as plain text corpora ignoring all the other information of which is enriched.
These algorithms turn to be ineffective in at least two cases: \emph{i)} when there is not enough information in the text of a page; \emph{ii)} when web pages have different content, but actually refer to the same semantic class.
The former case refers to web pages of poor of textual information, such as pages rich of structural data (e.g. pages from Deep Web Databases) or multimedia data, or when web pages have several script terms, which can be easily found also in other pages (e.g. pages from a CMS website). The latter case refers to web pages having the same semantic type (e.g. web pages related to professors, courses, books, lists of publications of a single researchers) but characterized by different distribution of terms. 
%The latter appear when page content can vary drastically across pages of one cluster 

On the other side, clustering based on structure typically considers the HTML formatting (i.e. HTML tags and visual information rendered by a web browser) \cite{Crescenzi:2005, Buttler:2004, Bohunsky:2010}. Algorithms, which use these information to organize web pages, are based on idea that web pages are automatically generated by programs that extract data from a back-end database and embed them into an HTML template. Web pages generated with this approach, show a common structure and layout, while differing in content. 
However, because tags are more often used for styling purposes than for content structuring, it can happen that most web pages in a website have the same structure, even if they refer to distinct semantic types. The effect is that the clustering algorithm will generate very few low-quality clusters.


These two solutions, however, only exploit within-page information. On the contrary, other solutions also exploit the graph  defined by the hyperlink structure of a set of web pages \cite{Lin:2010, Qi:2006}. 
Clustering on hyperlink structure is based on idea that web pages, differently from traditional textual documents, are enriched by hyperlinks that enable information to be split in multiple and
interdependent web pages. These hyperlinks can be used to identify collections of web pages semantically related and relationships among these collections. For example, the website of a computer science department may organize its web pages in collections of courses, research areas, and people; people may be further split into faculty, staff and students. Analyzing the link structure allow us to discover the hidden structure of website, which can be used to aid the navigation and the understanding of a website.
However the effectiveness of clustering algorithms strongly depends by links density and presence of noisy links.

Although there are several works that focus on web pages clustering, most of them analyze contents, web page structure (i.e. HTML formatting) and hyperlink structure of a website almost independently, mainly because different sources of information use different data representations. Over the last decade, some researchers tried to combine different sources of information. For example, \cite{He:2002, Modha:2000, Wang:2002, Drost:2005, Angelova:2006, Lin:2010} combine content and hyperlink structure for web page clustering, \cite{Glover:2002, Halkidi:2003, Calado:2003,Qi:2006, Zhu:2007} combine content and hyperlink structure for classification and \cite{Crescenzi:2005} combines web page and hyperlink structure for clustering purposes. 

This chapter is intended to be a contribution in the direction of performing clustering of web pages in a website by combining information about content, web page structure and hyperlink structure of web pages.
This goal is achieved analyzing web pages' HTML formatting to extract from each page collections of links, called \emph{web lists}, which can be used generate a compact and noise-free representation of the website's graph. Then, the extracted hyperlink structure and content information of web pages are mapped in a single vector space representation which can be used by any traditional and best-performing clustering algorithms.

Specifically, the proposed approach is based on the idea that two web pages are similar if they have common terms (i.e. \textit{Bag of words hypothesis} \cite{Turney:2010}) and they share the same reachability properties in the website's graph.
%In order to consider reachability, the solution we propose is inspired by the concept of Distributional hypothesis for words in natural language processing . This concept was proposed for the first time by Harris  \cite{} and famously articulated by Firth \cite{} as ''\textit{You shall know a word by the company it keeps}\rq\rq. In the context of the Web we can translate that citation in ''\textit{You shall know a web page by the paths it keeps}\rq\rq. According to this hypothesis two web pages are similar if they are involved in the same paths in the website's graph.
In order to consider reachability, the solution I propose is inspired by the concept of Distributional hypothesis, initially defined for words in natural language processing (i.e. ''\textit{You shall know a word by the company it keeps}\rq\rq)%\cite{Harris:1954, Firth:1957} 
\cite{Firth:1957} and recently extended to generic objects \cite{Gornerup:2015}. In the context of the Web it is possible to translate that citation in ''\textit{You shall know a web page by the paths it keeps}\rq\rq. According to this hypothesis two web pages are similar if they are involved in the same paths in the website's graph.
%Moreover, since a website's graph contains noisy links (e.g. short-cut hyperlinks), we use the web page structure (i.e. HTML formatting) to prune useless edges and improve the clustering process. 


This chapter is organized as follows: In the next section related work on clustering of web pages are described. In Section \ref{2sec:method} the proposed solution is detailed. Results empirically evaluated in Section \ref{2sec:Experiments}. %We conclude with main findings of the paper and future work in Section \ref{sec:Conclusion}.


\color{black}

\section{Related Work and Motivations}
Many techniques have been used for clustering large corpora of web pages. In particular, existing algorithms are based on two main approaches: \emph{content analysis} and \emph{structure analysis}.

%Textual content is the most straightforward feature that onemay consider to use. However, due to the variety of uncontrolled noise in web pages, directly using a bag-of-words representation for all terms may not achieve top performance

As clarified before, the most important clustering algorithms based on \textbf{web pages contents} treat web pages as independent textual documents. This is the case of ~\cite{Anami:2014, Chehreghani:2008, Haveliwala:2002, Zamir:1998}, where the words distribution is used to discover appropriate groups of interrelated web pages. %The advantage of this assumption is that many off-the-shelf clustering tools can be directly applied to the problem. In general, they use classical text mining techniques based on vector space model which describes some statistic information on the content, such as word frequency or tf-idf scores. 
 The advantage of this approach is that many off-the-shelf clustering tools based on vector space model can be directly applied.
%The most important clustering algorithms based on web pages contents are developed under the vector-space model~\cite{Chehreghani:2008, Haveliwala:2002, Zamir:1998}. Traditional Text Mining techniques are then applied to group web pages according to their topics. 
However, these approaches fail to learn accurate models %on web pages 
due the uncontrolled and heterogeneous nature of web page contents.
In fact, traditional clustering algorithms are based on the assumption that textual documents share consistent writing styles, provide enough contextual information, are plain and completely unstructured, and are independent and identically distributed. 
These limitations are more obvious for clustering of web pages belonging to different websites or whose content is created in a collaborative manner. In this case, in fact,  web pages related to the same topic could be contextually different, contain similar information into web elements with different semantic rules (e.g., navigational menu, main content, calendar, tables, and logotypes) and different functionalities (e.g., links, buttons, images, anchor-texts, etc.)~\cite{Qi:2009}. 
%Differently from textual documents, web pages have multiple representations which provide different information. 



On the contrary of plain text documents, web pages are characterized by structural properties such as HTML tags, visual features and hyperlinks which define the structural representations of web pages. It as been proved (see \cite{Zhu:2007, Crescenzi:2005, Bohunsky:2010, Lin:2010}) that such structural information provide different and complementary information respect to a textual representation.
%From this point of view, clustering methods that ignore the web page structure may not be suitable. 

A different perspective is represented by clustering algorithms that exploit structural properties. They can be organized in two main categories: \emph{i)} clustering based on HTML formatting and \emph{ii)} clustering based on hyperlink structure.   
Clustering based on \textbf{HTML formatting} takes advantage of the structural and visual information embedded in HTML tags, which are usually ignored by plain text approaches. 
In the literature several works (see, for instance \cite{Buttler:2004,Helmer:2012}) propose to organize web pages according with their structural information. They are mainly based on the assumption that HTML documents can be treated as XML documents. 
However, differently from XML tags, HTML tags are oriented towards data visualization rather than data representation. The consequence is that  it can appear that web pages of the same semantic type (e.g., web pages of professors) are codified and rendered using different tags structures or, viceversa, different types of web pages are codified with same tags structure. For example, structured data can be coded with a HTML table ($<$table$>$) tag or HTML list ($<$ul$>$) and have similar appearance. This defaces the quality of generated clusters.

To overcome this limit, in \cite{Bohunsky:2010, Crescenzi:2005}, the authors proposed to use visual information associated with HTML tags. %In~\cite{} (Clustering of Web Pages based on Visual Similarity) HTML tags are translated in four disjoint classes: group (table, ul, html, body, div), row (tr, li, h1, h2, hr, p), column (e.g., td) and text (other tags). Web pages are converted using these tag classes and the similarity between web pages is computed through the tree edit distance on converted web pages.
Specifically, in~\cite{Bohunsky:2010} clustering is based only on visual properties of web pages. Goal of this approach is to group web pages that could not have similar content, could not share the same HTML structure, but just look visually similar. 
In~\cite{Crescenzi:2005}  % the layout and presentation 
layout and visual properties associated with HTML tags are used to characterize the structure of the whole web page, and collections of hyperlinks in a web page are used to find pages with similar structural representation. 
%pointing to similar type of pages in terms of layout, presentation and regularities.  

Although visual information and HTML tags can improve the quality of web page clustering, algorithms based on HTML formatting  work well for well-structured and homogeneous web pages. This limits their usage to web pages  belonging to the same website and automatically generated from a back-end database (e.g., Deep Web Databases) or generated from dedicated content management systems (CMSs).
%web developers use different HTML structures for rendering similar information or viceversa use same HTML structure to represent different types of web page.   



Clustering algorithms based on the \textbf{hyperlink structure} work on a interrelated collection of web pages. The basic idea is that when two web pages are connected via a link, there is some semantic relationship between them, which can be the basis for the partitioning of the collection into clusters. %Hyperlinks allow us to represent a website as a graph of web pages interdependent among them where clustering algorithms can  be used to discover the hidden structure of a website.
%Exploiting link information to enhance web page organization has been studied extensively in the research community~\cite{}.
In general, such methods (see \cite{Cristo:2003}) only use direct links among web pages and, on the base of them, they use/define some similarity measures (e.g., bibliographic coupling~\cite{Kessler:1963}, co-citation~\cite{Small:1973}, etc.). 
% bibliographic coupling~\cite{Kessler:1963} (number of common out-link in two pages)~\cite{Kessler:1963}, co-citation (number of common in-link in two pages)~\cite{Small:1973},  Amsler (a measure that combines both co-citation and bibliographic coupling)~\cite{Amsler:1972} and Companion (a measure that, given a page, returns web pages in its neighborhood having the highest authority scores)
%all the web pages in its vicinity graph with  the highest authority scores) % (give a page returns all web neighbor pages having the highest authority scores as the pages that are most related to the start page u.) 
%\cite{Dean:1999}. %(Combining link-based and content based methods for web documents).
However, as claimed in \cite{Fisher:2003}, algorithms based on the hyperlink structure work well when the hyperlink structure is dense and noise-free.
%In fact, clustering based on hyperlinks structure suffers from the facts that pages without sufficient in-links or  out-links  could  not  be  clustered  and  results  in poor cluster quality.
In fact, clustering based on the hyperlink structure returns low quality results for web pages without sufficient amount of in-links or out-links. Moreover, not all the links are equally important for the clustering process: web pages are often enriched with noisy hyperlinks such as hyperlinks used to enforce the web page authority in a link-based ranking scenario or with short-cut hyperlinks. To overcome these issues several algorithms combine content information with link information (see  ~\cite{He:2002, Modha:2000, Wang:2002, Drost:2005, Angelova:2006, Lin:2010}). %(see  ~\cite{He:2002, Modha:2000, Wang:2002, Drost:2005, Angelova:2006, Lin:2010, Helmer:2012}).
%The earliest methods for combining text and link information for the clustering process are proposed in \cite{Angelova:2006} proposed an approach to linked document clustering by means of iterative relaxation of cluster assignments on a linked graph.. The first method uses the link information in the neighbors of a node in order to bias the term weights in a document. Term weights which are common between a document and its neighbors are given more importance in the clustering process. The second method is based on idea that the probability of a document to be assigned to a cluster depend from the content-based feature vector and the its neighbors' cluster memberships.

In this context, the earliest methods (for web page clustering) that 
try to combine textual information with co-citation and bibliographic coupling measures are \cite{Modha:2000, Wang:2002, Drost:2005}. Additionally, \cite{He:2002} faced the problem of noisy links, considering only hyperlinks among topically similar web pages and co-cited web pages. 
In particular, it associates a weight which combines the content similarity and co-citation to each edge $(i,j)$ connecting the web pages $i$ and $j$ in the website graph. Afterwards, a traditional clustering algorithm based on graph partitioning is adopted. The method has two main limitations: $i)$ textual information are used only to weight links, then two web pages sharing same distributional properties but having low textual similarity cannot be clustered together; $ii)$ the graph clustering algorithm is NP-hard.
In~\cite{Angelova:2006} a relaxation-labeling-based algorithm which first clusters
documents based on their contents and then re-assigns the computed labels using the hyperlink structure among immediate neighbors is developed. To avoid the potential increase in the level of noise,
only a subset of good neighbors are considered. %In~\cite{Helmer:2012} two types of links are considered: explicit links and implicit links. Explicit links   



An alternative approach to better organize web pages belonging to the same website and remove noisy links is to filter link collections having similar visual and/or structural properties~\cite{Crescenzi:2005, Qi:2006, Weninger:2013, Lin:2010, Lanotte:2014}. However, in my knowledge only~\cite{Crescenzi:2005} and \cite{Lin:2010} use information in link collections to improve clustering results. In~\cite{Lin:2010} the authors propose a similarity measure obtained combining textual similarity, co-citation and bibliography-coupling similarity and \emph{in-page link-structure} similarity. In this way, two web pages have a similar in-page link-structure if they appear more time together in a link collection. However, each measure needs a different representation space and their combinations is an open issue.

Previous solutions consider only direct relationships among neighbors, without analyzing the global structure of the website graph. \cite{Zhou:2010, Gornerup:2015, Tang:2015, Perozzi:2014} claim that the similarity between two graph's nodes can be expressed in terms of similarity of their respective contexts, that is, how they share surrounding nodes (which could not necessary immediately neighbors). 
In~\cite{Zhou:2010}, the authors propose a graph clustering algorithm that focuses on topological structure of a network and node properties, which can be textual or relational.
%In particular, a random walk-based distance metric is used in an augmented graph where nodes from the original graph are connected to new nodes that represent vertex attributes.
A set of attribute nodes and attribute edges are added to the original graph. With such graph augmentation, the attribute similarity is transformed to vertex vicinity in the graph: two vertices which share an attribute value are connected by a common attribute vertex. Then, a neighborhood random walk model, which measures the vertex closeness on the augmented graph through both structure edges and attribute edges, unifies the two similarities. 
%It is proved that the random walk model provide a good proximity score among nodes in the graph. 
Although the algorithm is able to combine structural and content information using a common representation, it cannot be applied to data having numerical values (e.g. tf-idf) or categorical attributes with a huge amount of distinct values.


 

In \cite{Perozzi:2014, Tang:2015} two embedding methods are proposed (DeepWalk and Line, respectively). They  exploit neural networks to generate a low-dimensional and dense vector representation of graph's nodes. DeepWalk~\cite{Perozzi:2014} applies the skip-gram method on truncated random walks to encode long-range influences among graph's nodes. However, this approach is not able to capture the local graph structure (i.e. nodes which can be considered similar because are strongly connected).
Line~\cite{Tang:2015} optimizes an objective function that
incorporates both the local (i.e. direct neighbors) and global (i.e. neighbors of neighbors) network structures. However, while DeepWalk is able to consider influences of arbitrary length, Line is able only to capture influence of length two. Moreover, both methods ignore node attributes (e.g. textual content). Consequently, clustering based on generated embedding might be difficult in graphs without sufficient in-links or out-links, but characterized by rich textual contents.

%\color{red}
%\begin{itemize}
%\item Such networks are often noisy, as many of the links and content features may not be relevant to the classification process. In addition, different portions of the network may be better suited to different kinds of classification models. For example, some portions of the network may be better classified with structure, whereas other portions may be better classified with content. We need to design a classifier, which can make such decisions in a seamless way, so that the appropriate parts of the network may be used most effectively for the classification process. (On Node Classification in Dynamic Content-based Networks)
%\item 1.  On the other hand, text-based algorithms have problems of high dimensionality and identification of different languages. To combine the advantages of both text based and hyperlink based approach hybrid document clustering approaches have been proposed.
%\item 2. But the link based clustering algorithms suffers from the facts that pages without sufficient in-links or out-links could not be clustered and results in poor cluster quality. 
%\item  Moreover most hyperlinks among different domains are used to increase the rank score of web pages in search engines.
%\item graph clustering: clustering results contain densely connected components within clusters. However, such methods usually ignore vertex attributes in the clustering process. \cite{Zhou:2010}
%\end{itemize}



%Continuare di qui: 
%http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.3718&rep=rep1&type=pdf
%(http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.3718&rep=rep1&type=pdf)
%http://link.springer.com.sci-hub.io/chapter/10.1007%2F978-3-642-22543-7_72
%http://dl.acm.org/citation.cfm?id=1934519 (Clustering Large Attributed Graphs: An Efficient Incremental Approach)
%(A Review on Web Pages Clustering Techniques )
% (Density link-based methods for clustering web pages)
%An equally important, but less-recognized result of the new problem definition is the need to automatically organize pages of the site into clusters, such that a single, high-quality wrapper can be induced for each cluster. 

\section{Methodology}
\label{2sec:method}
\noindent As anticipated, the problem I consider is that of clustering web pages belonging to the same website by combining content, web page structure and  hyperlink structure in a vector space representation. To achieve this goal, the proposed solution  implements a four steps strategy: in the first step website crawling is performed. Crawling uses web pages' structure information and exploits web lists in order to mitigate problems coming from noisy links. The output of this phase is the website graph, where each node represents a single page and edges represent hyperlinks. In the second step, I generate a link vector by exploiting Random Walks extracted from the website's graph. In the third phase content vectors are generated. In the last phase, a unified representation of pages is generated and clustering is performed on such representation. 



\subsection{Website crawling}
\label{3Crawling}
A Website can be formally described as a direct  graph $G = (V,E) $, where $V$ is the set of web pages and $E$ is the set of hyperlinks. In most cases, the homepage $h$ of a website representing the website's  entry  page  allows  the  website  to  be  viewed  as  a rooted directed graph. 



As claimed in \cite{Crescenzi:2005} not all links are equally important to describe the website structure. In fact, a website is rich of noisy links, which may not be relevant to clustering process, such as hyperlinks used to enforce the web page authority in a link-based ranking scenario, short-cut hyperlinks, etc. . Moreover, the website structure is codified in navigational systems which provide a local  view  of  the  website organization. Navigational systems (e.g. menus, navbars, product lists) are implemented as hyperlink collections having same domain name and sharing layout and presentation properties.
In this respect, the solution I propose, based on the usage of web lists, has a twofold effect: from one side it guarantees that only urls useful to the clustering process are considered; on the other side, it allows the method to implicitly take into account the web page structure which is implicitly codified in the web lists available web pages \cite{Crescenzi:2005, Qi:2006, Weninger:2013, Lanotte:2014}. 
\color{red}
%Since the website structure is codified in navigational systems (e.g. menus, navbars, product lists), that is,  hyperlink collections having same domain name and sharing layout and presentation properties, we select from the original graph $G$ a subset of hyperlinks $E' \subseteq E$ and nodes $V' \subseteq V$ obtaining a sub-graph $G' = (V', E') $.% In this way we try to mitigate problems coming from noisy links.
\color{black}
The crawling algorithm is described in Algorithm~\ref{3alg:crawlingWebsite}.
In particular, starting from the homepage $h$, the method \textit{extractWebLists()} (see Algorithm \ref{chap2alg:extractWebLists}) is iteratively applied to extract url collections having same domain of $h$ and organized in \emph{web lists} (see Def.~\ref{def_chap2:list}). Only web pages included in web lists are further explored. 

\begin{figure}
\centering
\includegraphics*[scale=0.48]{./imgs/chap_4/weblists}
\caption{In red rectangles the web lists extracted from a web page taken from \textit{www.cs.illinois.edu} are highlighted }
\label{fig:1}
\end{figure}




Fig.\ref{fig:1} shows, in red boxes, web lists extracted from the homepage of a computer science department which will be used to guide the website crawler. Links in box A will be excluded because their domains are different from the homepage's domain.

\begin{algorithm}[tb]
\caption{crawlingWebsite(homepage)}
\label{3alg:crawlingWebsite}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{$//$ \textit{#1}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\newcommand{\extractWebLists}[1]{ \textit{extractWebLists}(#1); }
\newcommand{\filterNot}[1]{ \textit{filterNot}(#1); }
\newcommand{\dequeue}[1]{ \textit{dequeue}(#1); }
\newcommand{\getText}[1]{ \textit{getText}(#1); }
\newcommand{\contains}[1]{ \textit{contains}(#1) }
\newcommand{\add}[1]{ \textit{add}(#1); }
\newcommand{\enqueue}[1]{ \textit{enqueue}(#1); }
%\newcommand{\empty}[1]{ \textit{empty}(#1); }
\newcommand{\RETURN}[1]{\textbf{return} #1}

\REQUIRE URL homepage;
\ENSURE Set$<$(URL, URL)$>$ E; Set$<$(URL, String)$>$ V;
\STATE frontier = Set()
\STATE Q = Queue(homepage)
\REPEAT 
	\STATE currentPage = Q.\dequeue{}
	\STATE text = currentPage.\getText{}
	\STATE V.\add{(currentPage, text)}
	\STATE webLists = \extractWebLists{currentPage}
	\FORALL{list $\in$ webLists}
		\STATE pagesToAnalyze = list.\filterNot{page $\rightarrow$ frontier.\contains{page}}
		\STATE Q.\enqueue{pagesToAnalyze}
		\STATE frontier.\add{pagesToAnalyze}
		\FORALL{u $\in$ pagesToAnalyze}
		\STATE E.\add{(currentPage, u)}
		\ENDFOR

 	\ENDFOR
\UNTIL{!queue.empty()}
\RETURN {(V, E)}

\end{algorithmic}
\end{algorithm}
To identify from a web page the set of web lists Algorithm~\ref{chap2alg:extractWebLists} is used (see Section \ref{2ListExtraction}). Since our goal is to extract the navigation systems encoded in a web page which are charachterized by a simple and a very similar HTML structure, the normalized edit distance is used here to evaluate the structural similarity among web elements.

The output of website crawling step is the sub-graph $G' = (V', E')$, where $V'\subseteq V$ and $E' \subseteq E$, which will be used for link and content vectors generation steps.




\subsection{Link vectors generation through Random  Walks}
\label{sec:linkGen}
A random walk over a linked structure is based on the idea that the connections between nodes encode information about their correlations. Then, the effective semantic of any node in a graph is obtained by  analyzing how it is correlated to all other nodes. 
To capture and codify correlations among graph's nodes (i.e. web pages), which can be indirect, the Random Walk with Restart (RWR) approach is used. 
 
%RWR is a Markov chain describing the sequence of nodes visited by a random walker. In particular, a walker begins its path by selecting a random starting point $i$; then with probability ($1- \alpha$) it stochastically walks to a new, connected neighbor node or with probability $\alpha$ it restarts his walk from $i$.  
%Random walk model is one of the most successful techniques known to the academic communities [19]. This is because the proximity defined by the model yields the following benefits: (1) it captures the global structure of the graph[8], and (2) it captures multi-facet relationships between two nodes unlike traditional graph distances [21]. (per riferimenti vedi Fast and Exact Top-k Search for Random Walk with Restart).



RWR is a Markov chain describing the sequence of nodes (i.e. web pages) visited by a random walker: starting from a random  point $i$, with probability ($1- \alpha$) a walker stochastically walks to a new, connected neighbor node or, with probability $\alpha$, it restarts his walk from $i$. Algorithm~\ref{3alg:datasetGeneration} describes the generation process.



%In order to use the global structure to capture the correlation among nodes and group them in community, the generated random walks must be sufficiently long to gather enough information about the topology of the graph.
In order to use the topology of the graph for capturing correlations among nodes, the generated random walks must be sufficiently long to encode those information. However, on the other side, it is advisable to avoid the effect described in~\cite{Pons:2005}, that is, when the length of a random walk starting at vertex $i$ tends towards infinity, the probability of being on a vertex $j$ does not depend on the starting vertex $i$. In our solution, the 
length of the random walk, $rwrLength$, is defined by the user (following indications provided in ~\cite{Pons:2005}, in the experiments, I set $rwrLength = 10$).
%Then we can see a random walk as a document (i.e. topical unites) and each web page as word (i.e. topic indicators)


Inspired by the information retrieval universe, a web page can be seen as a word, that is, a topic indicator and each random walk as a document constituting the natural context of words (i.e. topical unity).
Then, continuing the information-retrieval metaphor, we can represent a collection of random walks as a document collection where topics intertwine and overlap. The idea is to apply any NLP or information retrieval algorithm which uses the distributional hypothesis on document's objects to extract new knowledge~\cite{Sahlgren:2008}. 



\begin{algorithm}[tb]
\caption{rwrGeneration(rwrLength, dbLength, G, $\alpha$)}
\label{3alg:datasetGeneration}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{$//$ \textit{#1}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\newcommand{\getRandomVertex}[1]{ \textit{getRandomVertex}(#1); }
\newcommand{\getNextRandomVertex}[1]{ \textit{getRandomOutlink}(#1); }
\newcommand{\add}[1]{ \textit{add}(#1); }
\newcommand{\RETURN}[1]{\textbf{return} #1}

\REQUIRE int rwrLength, int dbLength, Graph G, float $\alpha$;
\ENSURE List$<$List$<$URL$>>$ randomWalks;
	\FORALL{i $\in$ Range(0, dbLength)}
	    \STATE w = List()
	    \STATE w[0] = G.\getRandomVertex{}
		\FORALL{j $\in$ Range(1, rwrLength) }
		\STATE $\lambda$ = Math.random()
		\IF{$\lambda > \alpha$}
		\STATE w[j] = G.\getNextRandomVertex{w[j-1]}
		\ELSE 
		\STATE w[j] = w[0]
		\ENDIF
		\ENDFOR
		\STATE randomWalks.\add{w}
 	\ENDFOR
\RETURN randomWalks

\end{algorithmic}
\end{algorithm}
 

For this scope, I apply a state-of-art algorithm, the skip-gram model \cite{Mikolov:2013} to extract a vector space representations of web pages that encode the topological structure of the website. In the skip-gram model we are given a word $w$ in a corpus of words $V_W$ (in our case a web page $w$ belonging to random walks) and their contexts $c \in V_C$ (in our case web pages in random walks which appear before and after the web page $w$).


We consider the conditional probabilities $p(c|w)$, and given a random walks collection $Rws$ generated by Algorithm~\ref{3alg:datasetGeneration}, the goal is to set the parameters $\theta$ of $p(c|w; \theta)$ so to maximize the following probability:
\begin{equation} 
\operatornamewithlimits{argmax}_{\theta} \prod_{L \in Rws; w \in L}\left [\prod_{c \in C_L(w)} prox_L(w,c)\cdot p(c|w;\theta) \right ] 
\label{eq:opt}
\end{equation}
where $L$ is a random walk in $Rws$, $w$ is a web page in $L$ and $C_L(w) = \{w_{i-k}, \dots , w_{i-1}, w_{i+1}, \dots, w_{i+k}\}$ is the set of contexts of web page $w$ in the list $L$. 
Moreover,  $prox_L(w,c)$ represents the proximity between $w$ and $c \in C_L(w)$. This is necessary since the  skip-gram model gives more importance to the nearby context words than distant context words.

One approach for parameterizing the skip-gram model follows the neural-network language models literature, and models the conditional probability $p(c|w;\theta)$ using
soft-max:
\begin{equation}
\label{eq:softmax}
p(c|w;\theta) = \frac{e^{v_c \cdot v_w}}{\sum_{c' \in V_C} e^{v_{c'} \cdot v_w} }
\end{equation}
where $v_c$, $v_{c'}$ and  $v_w \in \mathbb{R}^d$ are vector representations for $c, c'$ and $w$ respectively ($d$ is defined by the user). Therefore, the optimization problem \eqref{eq:opt} leads to the identification of the web page and context matrices $W = \{v_{w_i}| w_i \in V_W \}$ and $C= \{v_{c_i}| c_i \in V_C \}$.  They are dependent each other and we only use $W$ to represent web pages (coherently with what proposed in \cite{Mikolov:2013} for words).


Equation~\ref{eq:softmax} is computationally expensive due the summation $\sum_{c' \in V_C}$. One way of making the computation more tractable is to replace the softmax with an \emph{hierarchical softmax}. It uses a binary tree representation where words (web pages in our case) are leaves and each node stores the relative probabilities of its child nodes. Using this representation it is possible to evaluate only  $\sim log_2(V_C)$ nodes rather than $V_C$ \cite{Mikolov:2013}.



An alternative approach to hierarchical softmax is represented by the softmax negative-sampling approach (SGNS). In this case the objective function can be formalized as follows:
\begin{equation} 
\operatornamewithlimits{argmax}_{\theta} \prod_{(w,c) \in D}p((w,c)\in D| c,w; \theta) \prod_{(w,c) \not \in D'}p((w,c)\in D| c,w; \theta)
\end{equation}
where:
\begin{itemize}
\item $D$ is the set of all web pages and context pairs which are extracted from the website; 
\item $D'$ is the set of random pairs, assuming they are not present in the web site (i.e. negative examples). In \cite{Mikolov:2013} authors suggest a number of negative examples of 5-20 pairs (for each $w$) for small corpus data and 2-5 pairs (for each $w$) for big corpus data;
\item $p((w,c) \in D|c,w; \theta) = \frac{1}{1 + e^{-v_c \cdot v_w}}$ is the probability that the pair $(w, c)$ comes from the corpus data;
\item $p((w,c) \not \in D|c,w; \theta) = 1 - p(D=1|c,w; \theta)$ is the probability that the pair $(w, c)$ does not come from the corpus data;
\end{itemize}

Therefore, given in input to skip-gram model a corpus data composed by the collection of random walks, it returns 
the matrix $W$ which embeds each web page into a dense and low-dimensional space $\mathbb{R}^d$.% Moreover the embeddings are learned in the way that web pages having similar neighborhood will have similar 


\subsection{Content vectors generation}
In this section I describe the process for generating a vector representation of web pages using textual information. 
On the contrary of from traditional documents, web pages are written in HTML and contain additional information, such as HTML tags, hyperlinks and anchor text or other than textual content visible in a web browser. To apply on web pages a bag-of-words representation we need to compute a preprocessing step, in which the following operations are performed:
\begin{itemize}
\item Remove HTML tags. However, I maintain terms in anchor, title and metadata since they contribute to better organize web pages \cite{Fathi:2004}
\item Unescape escaped characters;
\item Eliminate non-alphanumeric characters;
\item Eliminate too frequent ($>90\%$) and infrequent ($<5\%$) words; 
\end{itemize}
After preprocessing, each web page is converted in a plain textual document and we can apply the traditional \emph{TF-IDF} weighting schema to obtain a content-vector representation.
Due the uncontrolled and heterogeneous nature of web page contents, vector representation of web pages based on content is characterized by high-dimensional sparse data. To obtain a dense and low-dimensional space the Truncated SVD algorithm is applied. It is a low-rank matrix approximation based on random sampling~\cite{Halko:2011}. In particular, given the \emph{TF-IDF matrix} of size $|V'|\times n$  and the desired dimensionality of content vectors $m$, where $m << n$, the algorithm returns a denser and lower-dimensional matrix of size $|V'|\times m$. 


\subsection{Content-link coupled Clustering}
\label{sec:Clustering}
Once the  content vector $v_c \in \mathbb{R}^m$ and the link vector $v_l \in \mathbb{R}^d$ of each web page in $V'$ have been generated, the last step of the algorithm is to concatenate them in a new vector having dimension $m+d$. Before the concatenation step I normalize each vector with its Euclidean norm. In this way we ensure that components of $v_l$ having highest weights are as important as components of $v_c$ having highest weights.

The matrix generated by concatenation step preserves both structural and textual information and can be used in traditional clustering algorithms based on vector space model. In this study I consider K-MEANS~\cite{Jain:2010} and H-DBSCAN~\cite{Campello:2013} because they are well known and present several complementary properties (distance vs. density-based).


\section{Experiments}
\label{2sec:Experiments}
In order to empirically evaluate the proposed approach, I performed experiments on several real websites. Specifically, I used  four computer science department's websites: \emph{Illinois} (cs.illinois.edu), \emph{Princeton} (cs.princeton.edu), \emph{Oxford} (www.cs.ox.ac.ou), \emph{Stanford} (cs.stanford.edu). The motivation behind this choice is related to our competence in manually labelling pages belonging to this domain. This was necessary in order to create a ground truth for the evaluation of the clustering results. 


This evaluation has been performed in order to answer to specific research questions: 1) Which is the real contribution of combining content and hyperlink structure in a single vector space representation with respect to using only either textual content or hyperlink structure? 2) Which is the real contribution of exploiting web pages structure (i.e. HTML formatting) and, specifically, the role of using web lists to reduce noise and improve clustering results?




In Table~\ref{tab:websites} the dimension of each dataset is described. In particular, to correctly analyze the contribution of web lists in the clustering process, I compare only the web pages extracted both by crawling websites using web lists and by traditional crawling (first column of Table~\ref{tab:websites}). Moreover, I report the dimension of the edge set obtained with traditional crawling (second column) and crawling using web lists (third column). Finally the last column describes the number of clusters manually identified.

\begin{table}[]
\centering
\caption{Description of Websites}
\label{tab:websites}
\begin{tabular}{|l|l|l|l|l|}
\hline
Website  & \#pages & \#edges & \#edges using web lists &\#clusters \\
\hline
\hline
Illinois & 563 & 9415 & 5330 & 10  \\ \hline
Oxford & 3480 & 44526 & 35148 & 19\\ \hline
Stanford  & 167 & 12372 & 30087 & 10\\ \hline
Princeton  &  3132 & 122493 &  104585 & 16 \\ \hline
\end{tabular}
\end{table}

I evaluated the effectiveness of the proposed approach by using the following measures:
\begin{itemize}
\item Homogeneity~\cite{vMeasure}: each cluster should contain only data points that are members
of a single class. This measure is computed by calculating the conditional entropy
of the class distribution given the proposed clustering. It is bounded below by 0.0 and above by 1.0 (higher is better).
\item Completeness~\cite{vMeasure}: all of the data points that are members of a given class should be elements of the same cluster. Symmetrically to Homogeneity it is computed by the conditional entropy of the proposed
cluster distribution given the real class. It ranges between 0 and 1 (higher is better).
\item V-Measure~\cite{vMeasure}: harmonic mean between homogeneity and completeness. 
\item Adjusted Mutual Information (AMI): it is a variation of the Mutual Information MI.
$
MI = \sum_{i \in K} \sum_{j \in C} log \frac{ P(i,j)}{P(i)P(j))}
$ where C is the set of real classes, K is the set of learned clusters, $P(i,j)$ denotes the probability that a point belongs to both the real class $i$ and the learned cluster $j$ and $P(i)$ is the a priori probability that a point falls into $i$. However MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared. The Adjusted Mutual Information represents an adjustment of this metric to overcome this limitation.    
\item Adjusted Random Index (ARI)~\cite{ARI}: it represents a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.
$
RI = {(a + b)}/{\binom{n}{2}}
$
where $a$ is number of pairs of points that are in the same class and learned cluster and $b$ is number of pairs of points that belong to different class and learned cluster. As in the case of $AMI$, the $ARI$ is an adjustment which ensures to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical. 
\item Silhouette: it measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. 
%If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.
\end{itemize}




To response to the previous research questions I ran the proposed algorithm with different configurations depending on the crawling type (with or without web list constrains) and web pages' information used for the vectors generation. I enumerate the used configurations as follows: 
\begin{itemize}
\item \textit{Text}. I generate a vector space representation, having dimension $m=120$, using only web pages' textual information;
\item \textit{RW-List}. I generate a vector space representation of size $d=120$ using only hyperlink structure extracted by crawling the website using web lists. For this scope I ran the \emph{rwrGeneration} algorithm (see Section~\ref{sec:linkGen}) setting the input parameters to $\alpha = 1$, $rwrLength = 10$ and $dbLength = 100k$;
\item \textit{RW-NoList}. I generate a vector space representation of size $120$ using only the hyperlink structure obtained with traditional crawling. I ran  \emph{rwrGeneration}  with the same parameters of RW-List;
\item \textit{Comb-Lists}. I combine, as defined in Section \ref{sec:Clustering}, the content vector of size $m=60$ and hyperlink structure vector of size $d=60$ generated by crawling the website using web lists. For link vector generation I use the same parameters of other configurations.      
\item \textit{Comb-NoLists}. As in the Comb-Lists configuration I combine textual  and  hyperlink  structure of web pages in a single vector space representation having size $120$. However, on the contrary of Comb-Lists, I use traditional crawling.
\end{itemize}
Since the goal is not that of comparing clustering algorithms, I set for K-MEANS the parameter $K$ (i.e. total number of clusters to generate) to the number of real clusters, while I set for H-DBSCAN the \emph{minimal cluster size} parameter to 5. Finally, since at the best of my knowledge there is no work which uses the skip-gram model to analyze the topological structure of websites, I ran both of skip-gram versions (i.e hierarchical softmax and SGNS) for generating link vectors. However, since experiments do no show relevant defferences between the two approaches, I report only results for SGNS (setting the window size to 5).


Tables \ref{tab:illinois}, \ref{tab:princeton}, \ref{tab:oxford} and \ref{tab:stanford} present the main results. In general, the experiments show that best results are obtained combining textual information with hyperlink structure. This is more evident for Illinois and Oxford websites, where content and hyperlinks structure codify complementary information for clustering purpose. However, for the Stanford website using the textual information decreases the clustering performance.
The importance of combining content and hyperlink structure is confirmed by Nemenyi post-hoc test (see Figure~\ref{tab:Nemenyi}) and Wilcoxon signed Rank test (see Table~\ref{tab:Wilcoxon}). This behaviour is quite uniform for all the evaluation measures considered (see Table~\ref{tab:Wilcoxon}). 

For the last research question, results do not show a statistical contribution in the use of web lists for clustering purpose (see Figure~\ref{tab:Nemenyi} and Table~\ref{tab:Wilcoxon}). This can be motivated by the fact that analyzed websites are very well structured and poor of noisy links. This can be observed in Table~\ref{tab:websites}, where there is not a big difference in terms of edges number between the real web graph and that one extracted using web lists. However, as expected the Completeness is higher for Comb-Lists, confirming that clusters have higher ''precision'' in the case of crawling based on web lists (see Figure \ref{tab:Nemenyi} b).
\begin{landscape}

\begin{table*}[h]
\centering
\caption{Experimental result for Illinois's website}
\label{tab:illinois}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Configuation  & Clustering & Homogeneity & Completeness & V-Measure & ARI & AMI & Silhouette \\ \hline \hline
Text &  KMEANS & 0.84 & 0.62 & 0.71 & 0.4 & 0.61 & 0.33\\ \hline
Text &  H-DBSCAN & 0.72 & 0.53 & 0.61 & 0.4 & 0.5 & 0.21\\ \hline
\hline
RW-Lists  & KMEANS & 0.72 & 0.53 & 0.61 & 0.27 & 0.51 & 0.42 \\ \hline
%RW-Lists & h. softmax & KMEANS & 0.73 & 0.54 & 0.62 & 0.27 & 0.52 & 0.43 \\ \hline
RW-Lists &  H-DBSCAN & 0.81 & 0.47 & 0.6 & 0.18 & 0.43 & \textbf{0.43} \\ \hline
%RW-Lists & h. softmax & H-DBSCAN & 0.83 & 0.47 & 0.6 & 0.2 & 0.43 & \textbf{0.47} \\ \hline
RW-NoLists  &  KMEANS & 0.71 & 0.52 & 0.6 & 0.25 & 0.5 & 0.42 \\ \hline
%RW-NoLists  & h. softmax & KMEANS & 0.73 & 0.54 & 0.62 & 0.28 & 0.52 & 0.41\\ \hline
RW-NoLists  &  H-DBSCAN & 0.8 & 0.45 & 0.58 & 0.17 & 0.41 & 0.42\\ \hline
%RW-NoLists & h. softmax & H-DBSCAN & 0.81 & 0.46 & 0.59 & 0.18 & 0.42 & 0.44\\ \hline
\hline
Comb-Lists &  KMEANS & \textbf{0.9} & \textbf{0.69} & \textbf{0.78} & \textbf{0.54} & \textbf{0.68} & 0.4 \\ \hline
%Comb-Lists & h. softmax & KMEANS & 0.86 & 0.64 & 0.74 & 0.41 & 0.63 & 0.39 \\ \hline
Comb-Lists &  H-DBSCAN & 0.83 & 0.51 & 0.63 & 0.27 & 0.48 & 0.34 \\ \hline
%Comb-Lists & h. softmax & H-DBSCAN & 0.86 & 0.5 & 0.63 & 0.24 & 0.46 & 0.39 \\ \hline
Comb-NoLists &  KMEANS & 0.84 & 0.62 & 0.71 & 0.37 & 0.6 & 0.38\\ \hline
%Comb-NoLists & h. softmax & KMEANS & 0.89 & 0.66 & 0.76 & 0.42 & 0.65 & 0.39\\ \hline
Comb-NoLists & H-DBSCAN & 0.83 & 0.52 & 0.64 & 0.27 & 0.49 & 0.29\\ \hline
%Comb-NoLists & h. softmax & H-DBSCAN & 0.84 & 0.49 & 0.62 & 0.23 & 0.46 & 0.3\\ \hline

\end{tabular}
\end{table*}


\begin{table*}[h]
\centering
\caption{Experimental results for the Princeton's website}
\label{tab:princeton}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Configuation  & Clustering & Homogeneity & Completeness & V-Measure & ARI & AMI & Silhouette \\ \hline \hline
Text  &KMEANS & 0.71 & \textbf{0.59} & \textbf{0.64} & \textbf{0.68} & \textbf{0.58} & 0.21\\ \hline
Text  & H-DBSCAN & 0.36 & 0.31 & 0.34 & 0.12 & 0.28 & -0.21\\ \hline
\hline
RW-Lists &  KMEANS & 0.56 & 0.37 & 0.45 & 0.27 & 0.36 & 0.18\\ \hline
%RW-Lists & h.softmax & KMEANS & 0.42 & 0.26 & 0.32 & 0.12 & 0.25 & 0.16\\ \hline
RW-Lists &  H-DBSCAN & 0.49 & 0.3 & 0.37 & 0.12 & 0.26 & -0.05\\ \hline
%RW-Lists & h.softmax & H-DBSCAN & 0.51 & 0.3 & 0.37 & 0.12 & 0.25 & 0\\ \hline
RW-NoLists &  KMEANS & 0.55 & 0.36 & 0.43 & 0.24 & 0.35 & 0.15\\ \hline
%RW-NoLists & h.softmax & KMEANS & 0.48 & 0.3 & 0.37 & 0.16 & 0.29 & 0.15\\ \hline
RW-NoLists & H-DBSCAN & 0.48 & 0.3 & 0.37 & 0.1 & 0.26 & -0.09\\ \hline
%RW-NoLists & h.softmax & H-DBSCAN & 0.49 & 0.3 & 0.37 & 0.1 & 0.26 & -0.04\\ \hline
\hline
Comb-Lists &  KMEANS & 0.76 & 0.54 & 0.63 & 0.55 & 0.53 & 0.14\\ \hline
%Comb-Lists & h.softmax & KMEANS & 0.65 & 0.42 & 0.51 & 0.26 & 0.41 & 0.12\\ \hline
Comb-Lists &  H-DBSCAN & 0.47 & 0.52 & 0.49 & 0.36 & 0.45 & \textbf{0.37}\\ \hline
%Comb-Lists & h.oftmax & H-DBSCAN & 0.45 & 0.5 & 0.47 & 0.36 & 0.43 & 0.21\\ \hline
Comb-NoLists & KMEANS & \textbf{0.78} & 0.54 & \textbf{0.64} & 0.49 & 0.53 & 0.13\\ \hline
%Comb-NoLists & h.softmax & KMEANS & 0.7 & 0.45 & 0.55 & 0.31 & 0.44 & 0.11\\ \hline
Comb-NoLists &  H-DBSCAN & 0.47 & 0.52 & 0.49 & 0.37 & 0.45 & 0.38\\ \hline
%Comb-NoLists & h.softmax & H-DBSCAN & 0.45 & 0.32 & 0.37 & 0.06 & 0.28 & -0.15\\ \hline
\end{tabular}
\end{table*}

\begin{table*}[h]
\centering
\caption{Experimental results for the Oxford's website}
\label{tab:oxford}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Configuation  & Clustering & Homogeneity & Completeness & V-Measure & ARI & AMI & Silhouette \\ \hline \hline
Text & KMEANS & 0.74 & 0.6 & 0.66 & 0.48 & 0.59 & 0.25\\ \hline
Text & H-DBSCAN & 0.43 & 0.41 & 0.42 & 0.07 & 0.37 & -0.06\\ \hline
\hline
RW-Lists & KMEANS & 0.65 & 0.55 & 0.6 & 0.48 & 0.54 & 0.32\\ \hline
%RW-Lists & h.softmax & KMEANS & 0.58 & 0.48 & 0.53 & 0.45 & 0.47 & 0.31\\ \hline
RW-Lists &  H-DBSCAN & 0.6 & 0.44 & 0.51 & 0.26 & 0.41 & 0.22\\ \hline
%RW-Lists & h.softmax & H-DBSCAN & 0.58 & 0.41 & 0.48 & 0.23 & 0.37 & 0.17\\ \hline
RW-NoLists & KMEANS & 0.67 & 0.57 & 0.62 & 0.51 & 0.56 & \textbf{0.35}\\ \hline
%RW-NoLists & h.softmax & KMEANS & 0.6 & 0.49 & 0.54 & 0.44 & 0.48 & 0.32\\ \hline
RW-NoLists & H-DBSCAN & 0.6 & 0.45 & 0.51 & 0.27 & 0.41 & 0.18\\ \hline
%RW-NoLists & h.softmax & H-DBSCAN & 0.57 & 0.41 & 0.48 & 0.22 & 0.37 & 0.17\\ \hline
\hline
Comb-Lists &  KMEANS & 0.79 & 0.67 & 0.73 & \textbf{0.56} & 0.67 & 0.34\\ \hline
%Comb-Lists & h.softmax & KMEANS & 0.78 & 0.66 & 0.72 & 0.54 & 0.66 & 0.32\\ \hline
Comb-Lists & H-DBSCAN & 0.58 & 0.49 & 0.53 & 0.15 & 0.47 & 0.08\\ \hline
%Comb-Lists & h.softmax & H-DBSCAN & 0.65 & 0.52 & 0.58 & 0.25 & 0.5 & 0.17\\ \hline
Comb-NoLists &  KMEANS & \textbf{0.81} & \textbf{0.68} & \textbf{0.74} & 0.53 & \textbf{0.68} & 0.28\\ \hline
%Comb-NoLists & h.softmax & KMEANS & 0.79 & 0.67 & 0.72 & 0.56 & 0.66 & 0.31\\ \hline
Comb-NoLists & H-DBSCAN & 0.62 & 0.53 & 0.57 & 0.23 & 0.51 & 0.08\\ \hline
%Comb-NoLists & h.softmax & H-DBSCAN & 0.62 & 0.51 & 0.56 & 0.25 & 0.48 & 0.13\\ \hline
\end{tabular}
\end{table*}

\begin{table*}[h]
\centering
\caption{Experimental results for the Stanford's website}
\label{tab:stanford}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Configuation  & Clustering & Homogeneity & Completeness & V-Measure & ARI & AMI & Silhouette \\ \hline \hline
Text  & KMEANS & 0.37 & 0.43 & 0.39 & 0.08 & 0.28 & 0.3\\ \hline
Text  & H-DBSCAN & 0.18 & 0.62 & 0.28 & 0.07 & 0.16 & 0.43 \\ \hline
\hline
RW-Lists  & KMEANS & \textbf{0.59} & 0.58 & \textbf{0.58} & \textbf{0.27} & \textbf{0.52} & 0.31\\ \hline
%RW-Lists & h.softmax & KMEANS & 0.62 & 0.58 & 0.6 & 0.3 & 0.51 & 0.33\\ \hline
RW-Lists & H-DBSCAN & 0.28 & 0.4 & 0.33 & 0.1 & 0.22 & 0.15\\ \hline
%RW-Lists & h.softmax & H-DBSCAN & 0.1 & 0.54 & 0.17 & 0.02 & 0.08 & 0.39\\ \hline
RW-NoLists & KMEANS & 0.47 & 0.54 & 0.5 & 0.14 & 0.39 & 0.53\\ \hline
%RW-NoLists & h.softmax & KMEANS & 0.47 & 0.52 & 0.5 & 0.15 & 0.4 & 0.5\\ \hline
RW-NoLists &  H-DBSCAN & 0.34 & 0.6 & 0.43 & 0.13 & 0.29 & \textbf{0.55}\\ \hline
%RW-NoLists & h.softmax & H-DBSCAN & 0.36 & 0.58 & 0.45 & 0.13 & 0.3 & 0.54\\ \hline
\hline
Comb-Lists& KMEANS & 0.42 & 0.46 & 0.44 & 0.12 & 0.34 & 0.22\\ \hline
%Comb-Lists& h.softmax & KMEANS & 0.55 & 0.56 & 0.55 & 0.21 & 0.48 & 0.22\\ \hline
Comb-Lists&  H-DBSCAN & 0.21 & \textbf{0.63} & 0.31 & 0.07 & 0.17 & 0.46\\ \hline
%Comb-Lists& h.softmax & H-DBSCAN & 0.22 & \textbf{0.63} & 0.32 & 0.08 & 0.18 & 0.48\\ \hline
Comb-NoLists  & KMEANS & 0.53 & 0.56 & 0.54 & 0.17 & 0.46 & 0.35\\ \hline
%Comb-NoLists & h.softmax & KMEANS & 0.47 & 0.51 & 0.49 & 0.14 & 0.39 & 0.38\\ \hline
Comb-NoLists & H-DBSCAN & 0.34 & 0.51 & 0.4 & 0.12 & 0.28 & 0.27\\ \hline
%Comb-NoLists & h.softmax & H-DBSCAN & 0.41 & 0.49 & 0.45 & 0.13 & 0.34 & 0.35\\ \hline

\end{tabular}
\end{table*}

\begin{table*}[h]
\centering
\caption{Wilcoxon pairwise signed Rank tests. (+) indicates that the second model wins. (-) indicates that the first model wins. The results are highlighted in bold if the difference is statistically significant (at $p$-value=0.05). The tests have been performed by considering the results obtained with both hierarchical softmax and SGNS skip-gram models.}
\label{tab:Wilcoxon}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
&Homogeneity & Completeness & V-Measure & Adj Rand index & Adj Mutual info & Silhouette \\ \hline \hline
Text vs Comb & \textbf{(+) 0.000} & (-) 0.055 & \textbf{(+) 0.000} & (+) 0.342 & \textbf{(+) 0.003} & \textbf{(+) 0.020}\\ \hline
RW vs Comb & \textbf{(+) 0.002} & \textbf{(+) 0.000} & \textbf{(+) 0.000} & \textbf{(+) 0.000} & \textbf{(+) 0.000} & (+) 0.229\\ \hline
NoLists vs Lists & (-) 0.342 & (-) 0.970 & (-) 0.418 & (+) 0.659 & (+) 0.358 & (-) 0.362\\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[h]
\centering
  \begin{tabular}{@{}cc@{}}
    \hspace{-1cm}\includegraphics*[scale=0.5]{./imgs/chap_4/Homogeneity}&
    \includegraphics*[scale=0.5]{./imgs/chap_4/Completeness} \\
    \scriptsize{(a) Homogeneity}& \scriptsize{(b) Completeness}\\
    \hspace{-1cm}\includegraphics*[scale=0.5]{./imgs/chap_4/V-Measure}&
    \includegraphics*[scale=0.5]{./imgs/chap_4/Adj_Mutual_info} \\
    \scriptsize{(c) V-Measure}& \scriptsize{(d) AMI}\\
    \hspace{-1cm}\includegraphics*[scale=0.5]{./imgs/chap_4/Adj_Rand_index} &
    \includegraphics*[scale=0.5]{./imgs/chap_4/Silhouette}\\
    \scriptsize{(e) ARI}& \scriptsize{(f) Silhouette}\\
  \end{tabular}
  \caption{Results of the Nemenyi post-hoc test for the results in terms of Homogeneity, Completeness, V-Measure, AMI, ARI, Silhouette. Better algorithms are positioned on the right-hand side, and those that do not significantly differ in performance (at $p$-value=0.05) are connected with a line. The tests have been performed by considering the results obtained with both hierarchical softmax and SGNS skip-gram models.}
  \label{tab:Nemenyi}
\end{figure*}


\end{landscape}




